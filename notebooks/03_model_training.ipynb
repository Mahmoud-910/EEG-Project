{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146e6dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EEG Person Identification - CNN + RNN Model Training\n",
    "Hybrid architecture combining spatial-frequency and temporal features\n",
    "Author: [Your Name]\n",
    "Date: 2025\n",
    "\n",
    "Model Architecture:\n",
    "1. CNN Branch: Extracts spatial-frequency features from spectrograms\n",
    "2. RNN Branch: Captures temporal dynamics from raw EEG\n",
    "3. Fusion Layer: Combines both feature representations\n",
    "4. Classification: 109-class person identification\n",
    "\"\"\"\n",
    "\n",
    "#%% Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "#%% Configuration\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for model training\"\"\"\n",
    "    # Paths\n",
    "    PROCESSED_DATA_PATH = './data/processed/eeg_processed_data.h5'\n",
    "    MODEL_DIR = './models/'\n",
    "    RESULTS_DIR = './results/'\n",
    "    \n",
    "    # Model parameters\n",
    "    N_CLASSES = 109  # Number of subjects\n",
    "    N_CHANNELS = 64\n",
    "    SEQUENCE_LENGTH = 480  # 3 seconds * 160 Hz\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    VALIDATION_SPLIT = 0.15\n",
    "    TEST_SPLIT = 0.15\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "print(\"\\nConfiguration loaded!\")\n",
    "\n",
    "#%% Load Processed Data\n",
    "\n",
    "def load_processed_data(filepath):\n",
    "    \"\"\"\n",
    "    Load preprocessed data from HDF5 file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_epochs : ndarray, shape (n_samples, n_channels, n_timesteps)\n",
    "        Raw filtered EEG epochs\n",
    "    X_spectrograms : ndarray, shape (n_samples, n_channels, n_freq, n_time)\n",
    "        Spectrogram representations\n",
    "    y : ndarray\n",
    "        Subject labels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING PROCESSED DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with h5py.File(filepath, 'r') as hf:\n",
    "        X_epochs = hf['X_epochs'][:]\n",
    "        X_spectrograms = hf['X_spectrograms'][:]\n",
    "        y_subjects = hf['y_subjects'][:]\n",
    "        y_tasks = hf['y_tasks'][:]\n",
    "        \n",
    "        # Print metadata\n",
    "        print(\"\\nDataset Metadata:\")\n",
    "        for key, value in hf.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nData loaded successfully!\")\n",
    "    print(f\"  Epochs shape: {X_epochs.shape}\")\n",
    "    print(f\"  Spectrograms shape: {X_spectrograms.shape}\")\n",
    "    print(f\"  Subjects shape: {y_subjects.shape}\")\n",
    "    print(f\"  Unique subjects: {len(np.unique(y_subjects))}\")\n",
    "    print(f\"  Total samples: {len(y_subjects)}\")\n",
    "    \n",
    "    return X_epochs, X_spectrograms, y_subjects\n",
    "\n",
    "#%% Prepare Data for Training\n",
    "\n",
    "def prepare_data(X_epochs, X_spectrograms, y_subjects, config):\n",
    "    \"\"\"\n",
    "    Prepare and split data for training\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Data splits: train, validation, and test sets\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREPARING DATA FOR TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Encode labels (subjects 1-109 to 0-108)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_subjects)\n",
    "    y_categorical = to_categorical(y_encoded, num_classes=config.N_CLASSES)\n",
    "    \n",
    "    print(f\"\\nLabel encoding:\")\n",
    "    print(f\"  Original range: {y_subjects.min()} - {y_subjects.max()}\")\n",
    "    print(f\"  Encoded range: {y_encoded.min()} - {y_encoded.max()}\")\n",
    "    print(f\"  One-hot shape: {y_categorical.shape}\")\n",
    "    \n",
    "    # Split data: 70% train, 15% validation, 15% test\n",
    "    # First split: 70% train, 30% temp\n",
    "    X_epoch_train, X_epoch_temp, X_spec_train, X_spec_temp, y_train, y_temp = train_test_split(\n",
    "        X_epochs, X_spectrograms, y_categorical,\n",
    "        test_size=0.30, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Second split: 50% of temp (15% of total) for val and test each\n",
    "    X_epoch_val, X_epoch_test, X_spec_val, X_spec_test, y_val, y_test = train_test_split(\n",
    "        X_epoch_temp, X_spec_temp, y_temp,\n",
    "        test_size=0.50, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData splits:\")\n",
    "    print(f\"  Training set: {len(X_epoch_train)} samples ({len(X_epoch_train)/len(X_epochs)*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {len(X_epoch_val)} samples ({len(X_epoch_val)/len(X_epochs)*100:.1f}%)\")\n",
    "    print(f\"  Test set: {len(X_epoch_test)} samples ({len(X_epoch_test)/len(X_epochs)*100:.1f}%)\")\n",
    "    \n",
    "    # Reshape data for model inputs\n",
    "    # RNN input: (batch, timesteps, channels)\n",
    "    X_epoch_train = np.transpose(X_epoch_train, (0, 2, 1))\n",
    "    X_epoch_val = np.transpose(X_epoch_val, (0, 2, 1))\n",
    "    X_epoch_test = np.transpose(X_epoch_test, (0, 2, 1))\n",
    "    \n",
    "    # CNN input: (batch, height, width, channels) - treat channels as the \"image\"\n",
    "    # Current: (batch, channels, freq, time) -> (batch, freq, time, channels)\n",
    "    X_spec_train = np.transpose(X_spec_train, (0, 2, 3, 1))\n",
    "    X_spec_val = np.transpose(X_spec_val, (0, 2, 3, 1))\n",
    "    X_spec_test = np.transpose(X_spec_test, (0, 2, 3, 1))\n",
    "    \n",
    "    print(f\"\\nReshaped data:\")\n",
    "    print(f\"  RNN input (train): {X_epoch_train.shape}\")\n",
    "    print(f\"  CNN input (train): {X_spec_train.shape}\")\n",
    "    \n",
    "    return (X_epoch_train, X_spec_train, y_train,\n",
    "            X_epoch_val, X_spec_val, y_val,\n",
    "            X_epoch_test, X_spec_test, y_test,\n",
    "            label_encoder)\n",
    "\n",
    "#%% Build CNN+RNN Hybrid Model\n",
    "\n",
    "def build_hybrid_model(config, epoch_shape, spec_shape):\n",
    "    \"\"\"\n",
    "    Build hybrid CNN+RNN model for person identification\n",
    "    \n",
    "    Architecture:\n",
    "    1. RNN Branch: LSTM for temporal feature extraction\n",
    "    2. CNN Branch: Conv2D for spatial-frequency feature extraction\n",
    "    3. Fusion: Concatenate features from both branches\n",
    "    4. Classification: Dense layers with softmax output\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : Config\n",
    "        Configuration object\n",
    "    epoch_shape : tuple\n",
    "        Shape of RNN input (timesteps, channels)\n",
    "    spec_shape : tuple\n",
    "        Shape of CNN input (freq, time, channels)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled hybrid model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BUILDING HYBRID CNN+RNN MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ==================== RNN BRANCH ====================\n",
    "    # Input: (timesteps, channels)\n",
    "    rnn_input = layers.Input(shape=epoch_shape, name='rnn_input')\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    x_rnn = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.3)\n",
    "    )(rnn_input)\n",
    "    x_rnn = layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences=False, dropout=0.3)\n",
    "    )(x_rnn)\n",
    "    \n",
    "    # Dense layers for RNN branch\n",
    "    x_rnn = layers.Dense(128, activation='relu')(x_rnn)\n",
    "    x_rnn = layers.BatchNormalization()(x_rnn)\n",
    "    x_rnn = layers.Dropout(0.4)(x_rnn)\n",
    "    rnn_output = layers.Dense(64, activation='relu', name='rnn_features')(x_rnn)\n",
    "    \n",
    "    print(\"\\n✓ RNN Branch built:\")\n",
    "    print(\"  - Bidirectional LSTM (128 units)\")\n",
    "    print(\"  - Bidirectional LSTM (64 units)\")\n",
    "    print(\"  - Dense layers with dropout\")\n",
    "    \n",
    "    # ==================== CNN BRANCH ====================\n",
    "    # Input: (freq, time, channels)\n",
    "    cnn_input = layers.Input(shape=spec_shape, name='cnn_input')\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    x_cnn = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(cnn_input)\n",
    "    x_cnn = layers.BatchNormalization()(x_cnn)\n",
    "    x_cnn = layers.MaxPooling2D((2, 2))(x_cnn)\n",
    "    x_cnn = layers.Dropout(0.3)(x_cnn)\n",
    "    \n",
    "    x_cnn = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x_cnn)\n",
    "    x_cnn = layers.BatchNormalization()(x_cnn)\n",
    "    x_cnn = layers.MaxPooling2D((2, 2))(x_cnn)\n",
    "    x_cnn = layers.Dropout(0.3)(x_cnn)\n",
    "    \n",
    "    x_cnn = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x_cnn)\n",
    "    x_cnn = layers.BatchNormalization()(x_cnn)\n",
    "    x_cnn = layers.MaxPooling2D((2, 2))(x_cnn)\n",
    "    x_cnn = layers.Dropout(0.4)(x_cnn)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x_cnn = layers.GlobalAveragePooling2D()(x_cnn)\n",
    "    \n",
    "    # Dense layers for CNN branch\n",
    "    x_cnn = layers.Dense(128, activation='relu')(x_cnn)\n",
    "    x_cnn = layers.BatchNormalization()(x_cnn)\n",
    "    x_cnn = layers.Dropout(0.4)(x_cnn)\n",
    "    cnn_output = layers.Dense(64, activation='relu', name='cnn_features')(x_cnn)\n",
    "    \n",
    "    print(\"\\n✓ CNN Branch built:\")\n",
    "    print(\"  - 3 Convolutional blocks (32, 64, 128 filters)\")\n",
    "    print(\"  - Batch normalization and dropout\")\n",
    "    print(\"  - Global average pooling\")\n",
    "    \n",
    "    # ==================== FUSION & CLASSIFICATION ====================\n",
    "    # Concatenate features from both branches\n",
    "    merged = layers.concatenate([rnn_output, cnn_output], name='fusion')\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dense(256, activation='relu')(merged)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Output layer: 109 subjects\n",
    "    output = layers.Dense(config.N_CLASSES, activation='softmax', name='output')(x)\n",
    "    \n",
    "    print(\"\\n✓ Fusion & Classification layers built:\")\n",
    "    print(\"  - Concatenated feature dimension: 128\")\n",
    "    print(\"  - Dense layers (256, 128)\")\n",
    "    print(\"  - Output layer: 109 classes (subjects)\")\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[rnn_input, cnn_input], outputs=output, name='CNN_RNN_Hybrid')\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Model compiled with:\")\n",
    "    print(f\"  - Optimizer: Adam (lr={config.LEARNING_RATE})\")\n",
    "    print(f\"  - Loss: Categorical Crossentropy\")\n",
    "    print(f\"  - Metrics: Accuracy, Top-5 Accuracy\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "#%% Training Callbacks\n",
    "\n",
    "def create_callbacks(config):\n",
    "    \"\"\"\n",
    "    Create training callbacks\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    callbacks : list\n",
    "        List of Keras callbacks\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        # Save best model\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(config.MODEL_DIR, 'best_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Early stopping\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate on plateau\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        TensorBoard(\n",
    "            log_dir=os.path.join(config.RESULTS_DIR, 'logs'),\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "#%% Train Model\n",
    "\n",
    "def train_model(model, X_epoch_train, X_spec_train, y_train,\n",
    "                X_epoch_val, X_spec_val, y_val, config):\n",
    "    \"\"\"\n",
    "    Train the hybrid model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : History object\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    callbacks = create_callbacks(config)\n",
    "    \n",
    "    print(f\"\\nStarting training:\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Epochs: {config.EPOCHS}\")\n",
    "    print(f\"  Training samples: {len(X_epoch_train)}\")\n",
    "    print(f\"  Validation samples: {len(X_epoch_val)}\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_epoch_train, X_spec_train],\n",
    "        y_train,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        epochs=config.EPOCHS,\n",
    "        validation_data=([X_epoch_val, X_spec_val], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "#%% Plot Training History\n",
    "\n",
    "def plot_training_history(history, config):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PLOTTING TRAINING HISTORY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Accuracy\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Top-5 Accuracy\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(history.history['top5_accuracy'], label='Train Top-5', linewidth=2)\n",
    "    ax3.plot(history.history['val_top5_accuracy'], label='Val Top-5', linewidth=2)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Top-5 Accuracy')\n",
    "    ax3.set_title('Top-5 Accuracy')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Learning Rate\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'lr' in history.history:\n",
    "        ax4.plot(history.history['lr'], linewidth=2, color='coral')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Learning Rate')\n",
    "        ax4.set_title('Learning Rate Schedule')\n",
    "        ax4.set_yscale('log')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Learning rate not logged', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.RESULTS_DIR, 'training_history.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\"\\n✓ Training history plot saved!\")\n",
    "    plt.show()\n",
    "\n",
    "#%% Save Training Results\n",
    "\n",
    "def save_training_results(history, model, config):\n",
    "    \"\"\"\n",
    "    Save training history and model summary\n",
    "    \"\"\"\n",
    "    # Save history to CSV\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(config.RESULTS_DIR, 'training_history.csv'), index=False)\n",
    "    \n",
    "    # Save model summary\n",
    "    with open(os.path.join(config.RESULTS_DIR, 'model_summary.txt'), 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    \n",
    "    print(\"\\n✓ Training results saved!\")\n",
    "\n",
    "#%% Main Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CNN+RNN HYBRID MODEL - TRAINING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    X_epochs, X_spectrograms, y_subjects = load_processed_data(config.PROCESSED_DATA_PATH)\n",
    "    \n",
    "    # Prepare data\n",
    "    (X_epoch_train, X_spec_train, y_train,\n",
    "     X_epoch_val, X_spec_val, y_val,\n",
    "     X_epoch_test, X_spec_test, y_test,\n",
    "     label_encoder) = prepare_data(X_epochs, X_spectrograms, y_subjects, config)\n",
    "    \n",
    "    # Build model\n",
    "    epoch_shape = X_epoch_train.shape[1:]  # (timesteps, channels)\n",
    "    spec_shape = X_spec_train.shape[1:]    # (freq, time, channels)\n",
    "    \n",
    "    model = build_hybrid_model(config, epoch_shape, spec_shape)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    model.summary()\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = model.count_params()\n",
    "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(model, X_epoch_train, X_spec_train, y_train,\n",
    "                         X_epoch_val, X_spec_val, y_val, config)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, config)\n",
    "    \n",
    "    # Save results\n",
    "    save_training_results(history, model, config)\n",
    "    \n",
    "    # Save test data for evaluation\n",
    "    np.savez(os.path.join(config.RESULTS_DIR, 'test_data.npz'),\n",
    "             X_epoch_test=X_epoch_test,\n",
    "             X_spec_test=X_spec_test,\n",
    "             y_test=y_test)\n",
    "    \n",
    "    # Save label encoder\n",
    "    import pickle\n",
    "    with open(os.path.join(config.RESULTS_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest model saved to: {os.path.join(config.MODEL_DIR, 'best_model.h5')}\")\n",
    "    print(f\"Results saved to: {config.RESULTS_DIR}\")\n",
    "    print(\"\\nNext step: Run 04_evaluation_visualization.ipynb\")\n",
    "\n",
    "#%% Training Summary\n",
    "\n",
    "\"\"\"\n",
    "MODEL TRAINING SUMMARY\n",
    "======================\n",
    "\n",
    "Hybrid CNN+RNN Architecture:\n",
    "- RNN Branch: Bidirectional LSTM (128→64 units) for temporal features\n",
    "- CNN Branch: 3 Conv2D blocks (32, 64, 128 filters) for spatial-frequency features\n",
    "- Fusion: Concatenated features (128 dimensions)\n",
    "- Classification: Dense layers (256, 128) → 109-class softmax\n",
    "\n",
    "Training Configuration:\n",
    "- Optimizer: Adam (lr=0.001 with ReduceLROnPlateau)\n",
    "- Loss: Categorical Crossentropy\n",
    "- Metrics: Accuracy, Top-5 Accuracy\n",
    "- Batch size: 32\n",
    "- Epochs: 100 (with early stopping)\n",
    "- Data split: 70% train, 15% val, 15% test\n",
    "\n",
    "Regularization:\n",
    "- Dropout layers (0.3-0.5)\n",
    "- Batch normalization\n",
    "- Early stopping (patience=15)\n",
    "- Learning rate reduction (patience=7)\n",
    "\n",
    "Output Files:\n",
    "- best_model.h5: Best model weights\n",
    "- training_history.csv: Training metrics\n",
    "- training_history.png: Training plots\n",
    "- model_summary.txt: Model architecture\n",
    "- test_data.npz: Test set for evaluation\n",
    "- label_encoder.pkl: Label encoder for decoding\n",
    "\n",
    "The model is now ready for evaluation!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
